---
title: 'TITLE HERE'
author: "YOUR NAME HERE"
date: "July 13, 2020"
subtitle: Health Analytics Challenge Lab 2020
output:
  html_document:
    toc: yes
    toc_depth: 2
---

# Dependencies for running the Rmd
Code is mostly excluded from the knit .html version of this notebook to maintain a clean presentation. It is included in a few places that make sense for demonstrative purposes. The full code is provided in the accompanying .Rmd file.

To run the .Rmd file, make sure the included dependencies Elasticsearch.R and elasticsearch_queries.R are in the same directory as the Rmd, and make sure to set "elasticsearch_host" to the approriate value here (this is not included in the github version for security reasons).

```{r}
elasticsearch_host <- "lp01.idea.rpi.edu"
```

#  Overview

Discuss the motivation of this twitter study and what it aims to accomplish. Specifically, what is the research question that is being investigated?

# Methodology

##The dataset

Provide details on the dataset being used in this study. Include details on how the dataset was collected and provide references/citations if applicable.

If using the Rensselaer IDEA COVID-TweetIDs dataset, it can be referenced here:
[Rensselaer IDEA COVID-19 Tweet Dataset](https://github.com/TheRensselaerIDEA/COVID-TweetIDs)

If using the dataset from the paper "Extracting COVID-19 Events from Twitter", it can be referenced here:
[Extracting COVID-19 Events from Twitter](https://arxiv.org/abs/2006.02567)

## Analysis methods

Discuss how the data is being used to answer the research question. Provide details on any statistical methods, aggregations, classification, clustering, etc. being used.

# Results

Here, run the code to query the dataset from the appropriate elasticsearch index, execute the analysis, and visualize the results.

```{r setup, include=FALSE}
# Required R package installation:
# These will install packages if they are not already installed
# Set the correct default repository
r = getOption("repos")
r["CRAN"] = "http://cran.rstudio.com"
options(repos = r)

if (!require("ggplot2")) {
  install.packages("ggplot2")
  library(ggplot2)
}

if (!require("knitr")) {
  install.packages("knitr")
  library(knitr)
}

if(!require('dplyr')) {
  install.packages("dplyr")
  library(dplyr)
}

if(!require('stringr')) {
  install.packages("stringr")
  library(stringr)
}

if(!require('Rtsne')) {
  install.packages("Rtsne")
  library(Rtsne)
}

if(!require('stopwords')) {
  install.packages("stopwords")
  library(stopwords)
}

if(!require('plotly')) {
  install.packages("plotly")
  library(plotly)
}

if (!require("kableExtra")) {
  install.packages("kableExtra")
  library(kableExtra)
}

knitr::opts_chunk$set(echo = TRUE)

source("Elasticsearch.R")
```

## Query setup
```{r}
# query start date/time (inclusive)
rangestart <- "2020-01-01 00:00:00"

# query end date/time (exclusive)
rangeend <- "2020-08-01 00:00:00"

# text filter restricts results to only those containing words, phrases, or meeting a boolean condition. This query syntax is very flexible and supports a wide variety of filter scenarios:
# words: text_filter <- "cdc nih who"  ...contains "cdc" or "nih" or "who"
# phrase: text_filter <- '"vitamin c"' ...contains exact phrase "vitamin c"
# boolean condition: <- '(cdc nih who) +"vitamin c"' ...contains ("cdc" or "nih" or "who") and exact phrase "vitamin c"
#full specification here: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html
text_filter <- ""

# location filter acts like text filter except applied to the location of the tweet instead of its text body.
location_filter <- ""

# if FALSE, location filter considers both user-povided and geotagged locations. If TRUE, only geotagged locations are considered.
must_have_geo <- FALSE

# query semantic similarity phrase
semantic_phrase <- ""

# return results in chronological order or as a random sample within the range
# (ignored if semantic_phrase is not blank)
random_sample <- FALSE
# if using random sampling, optionally specify a seed for reproducibility. For no seed, set to NA.
random_seed <- NA
# number of results to return (to return all results, set to NA)
resultsize <- 10000
# minimum number of results to return. This should be set according to the needs of the analysis (i.e. enough samples for statistical significance)
min_results <- 500
```

```{r, echo=FALSE}
###############################################################################
# Get the tweets from Elasticsearch using the search parameters defined above
###############################################################################

elasticsearch_indexname <- "covidevents-data"

results <- do_search(indexname=elasticsearch_indexname, 
                     rangestart=rangestart,
                     rangeend=rangeend,
                     text_filter=text_filter,
                     location_filter=location_filter,
                     semantic_phrase=semantic_phrase,
                     must_have_embedding=TRUE,
                     must_have_geo=must_have_geo,
                     random_sample=random_sample,
                     random_seed=random_seed,
                     resultsize=resultsize,
                     resultfields='"user.screen_name", "user.verified", "user.location", "place.full_name", "place.country", "text", "full_text", "extended_tweet.full_text", "embedding.use_large.primary", "dataset_file", "dataset_entry.annotation.part1.Response", "dataset_entry.annotation.part2-opinion.Response"',
                     elasticsearch_host=elasticsearch_host,
                     elasticsearch_path="elasticsearch",
                     elasticsearch_port=443,
                     elasticsearch_schema="https")

# this dataframe contains the tweet text and other metadata
required_fields <- c("full_text", "user_screen_name", "user_verified", "user_location", "place.country", "place.full_name", "dataset_file", "dataset_entry.annotation.part1.Response", "dataset_entry.annotation.part2-opinion.Response")
validate_results(results$df, min_results, required_fields)
tweet.vectors.df <- results$df[,required_fields]

# this matrix contains the embedding vectors for every tweet in tweet.vectors.df
tweet.vectors.matrix <- t(simplify2array(results$df[,"embedding.use_large.primary"]))
```

```{r, echo=FALSE}
###############################################################################
# Clean the tweet and user location text, and set up tweet.vectors.df 
# the way we want it by consolidating the location field and computing
# location type
###############################################################################

tweet.vectors.df$user_location <- ifelse(is.na(tweet.vectors.df$place.full_name), tweet.vectors.df$user_location, paste(tweet.vectors.df$place.full_name, tweet.vectors.df$place.country, sep=", "))
tweet.vectors.df$user_location[is.na(tweet.vectors.df$user_location)] <- ""
tweet.vectors.df$user_location_type <- ifelse(is.na(tweet.vectors.df$place.full_name), "User", "Place")
tweet.vectors.df$class <- sapply(tweet.vectors.df$dataset_file, function(d) sub(".jsonl", "", d))
colnames(tweet.vectors.df)[colnames(tweet.vectors.df) == "dataset_entry.annotation.part1.Response"] <- "is_specific_event"
colnames(tweet.vectors.df)[colnames(tweet.vectors.df) == "dataset_entry.annotation.part2-opinion.Response"] <- "opinion"
tweet.vectors.df <- tweet.vectors.df[, c("full_text", "user_screen_name", "user_verified", "user_location", "user_location_type", "class", "is_specific_event", "opinion")]

clean_text <- function(text, for_freq=FALSE) {
  text <- str_replace_all(text, "[\\s]+", " ")
  text <- str_replace_all(text, "http\\S+", "")
  if (isTRUE(for_freq)) {
    text <- tolower(text)
    text <- str_replace_all(text, "’", "'")
    text <- str_replace_all(text, "_", "-")
    text <- str_replace_all(text, "[^a-z1-9 ']", "")
  } else {
    text <- str_replace_all(text, "[^a-zA-Z1-9 `~!@#$%^&*()-_=+\\[\\];:'\",./?’]", "")
  }
  text <- str_replace_all(text, " +", " ")
  text <- trimws(text)
}
tweet.vectors.df$full_text <- sapply(tweet.vectors.df$full_text, clean_text)
tweet.vectors.df$user_location <- sapply(tweet.vectors.df$user_location, clean_text)
```

## Selection of optimal number of clusters and subclusters

To find the optimal number of high-level theme clusters for this sample, an elbow plot is used:

```{r, echo=FALSE}
wssplot <- function(data, fc=1, nc=40, seed=20){
  wss <- data.frame(k=fc:nc, withinss=c(0))
  for (i in fc:nc){
    set.seed(seed)
    wss[i-fc+1,2] <- sum(kmeans(data, centers=i, iter.max=30)$withinss)}
  ggplot(data=wss,aes(x=k,y=withinss)) + 
    geom_line() + 
    ggtitle("Quality (within sums of squares) of k-means by choice of k")
}
# Generate the plot
wssplot(tweet.vectors.matrix)
```

The plot mostly represents a smooth curve, although there is a distinct "elbow" point between k=8 and k=10. We will select k=8:

```{r}
k <- 8
```

```{r, echo=FALSE}
###############################################################################
# Run K-means on all the tweet embedding vectors
###############################################################################

set.seed(300)
km <- kmeans(tweet.vectors.matrix, centers=k, iter.max=30)

tweet.vectors.df$vector_type <- factor("tweet", levels=c("tweet", "cluster_center", "subcluster_center"))
tweet.vectors.df$cluster <- as.factor(km$cluster)

#append cluster centers to dataset for visualization
centers.df <- data.frame(full_text=paste("Cluster (", rownames(km$centers), ") Center", sep=""),
                         user_screen_name="[N/A]",
                         user_verified="[N/A]",
                         user_location="[N/A]",
                         user_location_type = "[N/A]",
                         class = "[N/A]",
                         is_specific_event = "[N/A]",
                         opinion = "[N/A]",
                         vector_type = "cluster_center",
                         cluster=as.factor(rownames(km$centers)))
tweet.vectors.df <- rbind(tweet.vectors.df, centers.df)
tweet.vectors.matrix <- rbind(tweet.vectors.matrix, km$centers)
```

To find the optimal number of topic subclusters for each theme cluster, another elbow plot is generated with a separate curve for each theme cluster. Since the within sums of squares can be on different scales for theme clusters of different sizes and levels of diversity, the withinss metric is scaled to 0 mean and unit variance: 

```{r, echo=FALSE}
wssplot2 <- function(data, fc=1, nc=40, seed=20){
  clusters <- max(data[,1])
  wss <- data.frame(cluster=as.factor(sort(rep(1:clusters, nc-fc+1))), k=rep(fc:nc, clusters), withinss=c(0))
  for (i in 1:clusters) {
    for (j in fc:nc){
      set.seed(seed)
      wss[wss$cluster==i,][j,"withinss"] <- sum(kmeans(data[data[,1]==i,2:ncol(data)], centers=j, iter.max=30)$withinss)
      }
  }
  wss$withinss.scaled <- unlist(lapply(1:clusters, function(n) scale(wss$withinss[wss$cluster==n])))
  ggplot(data=wss,aes(x=k,y=withinss.scaled)) + 
    geom_line(aes(color=cluster, linetype=cluster)) + 
    ggtitle("Quality (scaled within sums of squares) of k-means by choice of k")
}
# Generate the plot
wssplot2(cbind(tweet.vectors.df$cluster, tweet.vectors.matrix))
```

Each theme cluster follows a similar plot, again representing a smooth curve. This time there is no clear "elbow" point. A reasonable choice of k can be selected anywhere between 8 and 15. We will select cluster.k=8 for the topic subclusters:

```{r}
cluster.k <- 8
```

## Visualization of theme clusters and topic subclusters

```{r, echo=FALSE}
###############################################################################
# Run K-means again on all the tweet embedding vectors in each cluster
# to create subclusters of tweets
###############################################################################

tweet.vectors.df$subcluster <- c(0)

for (i in 1:k){
 print(paste("Subclustering cluster", i, "..."))
 cluster.matrix <- tweet.vectors.matrix[tweet.vectors.df$cluster == i,]
 set.seed(500)
 cluster.km <- kmeans(cluster.matrix, centers=cluster.k, iter.max=30)
 tweet.vectors.df[tweet.vectors.df$cluster == i, "subcluster"] <- cluster.km$cluster
 
 #append subcluster centers to dataset for visualization
 centers.df <- data.frame(full_text=paste("Subcluster (", rownames(cluster.km$centers), ") Center", sep=""),
                         user_screen_name="[N/A]",
                         user_verified="[N/A]",
                         user_location="[N/A]",
                         user_location_type = "[N/A]",
                         class = "[N/A]",
                         is_specific_event = "[N/A]",
                         opinion = "[N/A]",
                         vector_type = "subcluster_center",
                         cluster=as.factor(i),
                         subcluster=rownames(cluster.km$centers))
 tweet.vectors.df <- rbind(tweet.vectors.df, centers.df)
 tweet.vectors.matrix <- rbind(tweet.vectors.matrix, cluster.km$centers)
}
tweet.vectors.df$subcluster <- as.factor(tweet.vectors.df$subcluster)
```

```{r, echo=FALSE}
###############################################################################
# Compute labels for each cluster and subcluster based on word frequency
# and identify the nearest neighbors to each cluster and subcluster center
###############################################################################

stop_words <- stopwords("en", source="snowball")
stop_words <- union(stop_words, stopwords("en", source="nltk"))
stop_words <- union(stop_words, stopwords("en", source="smart"))
stop_words <- union(stop_words, stopwords("en", source="marimo"))
stop_words <- union(stop_words, c(",", ".", "!", "-", "?", "&amp;", "amp"))

get_word_freqs <- function(full_text) {
  word_freqs <- table(unlist(strsplit(clean_text(full_text, TRUE), " ")))
  word_freqs <- cbind.data.frame(names(word_freqs), as.integer(word_freqs))
  colnames(word_freqs) <- c("word", "count")
  word_freqs <- word_freqs[!(word_freqs$word %in% stop_words),]
  word_freqs <- word_freqs[order(word_freqs$count, decreasing=TRUE),]
}

get_label <- function(word_freqs, exclude_from_labels=NULL, top_k=3) {
  words <- as.character(word_freqs$word)
  exclude_words <- NULL
  if (!is.null(exclude_from_labels)) {
    exclude_words <- unique(unlist(lapply(strsplit(exclude_from_labels, "/"), trimws)))
  }
  label <- paste(setdiff(words, exclude_words)[1:top_k], collapse=" / ")
}

get_nearest_center <- function(df, mtx, center) {
  df$center_cosine_similarity <- apply(mtx, 1, function(v) (v %*% center)/(norm(v, type="2")*norm(center, type="2")))
  nearest_center <- df[order(df$center_cosine_similarity, decreasing=TRUE),]
  nearest_center <- nearest_center[nearest_center$vector_type=="tweet", c("center_cosine_similarity", "full_text", "user_location")]
}

master.word_freqs <- get_word_freqs(tweet.vectors.df$full_text)
master.label <- get_label(master.word_freqs, top_k=6)

clusters <- list()
for (i in 1:k) {
  cluster.df <- tweet.vectors.df[tweet.vectors.df$cluster == i,]
  cluster.matrix <- tweet.vectors.matrix[tweet.vectors.df$cluster == i,]
    
  cluster.word_freqs <- get_word_freqs(cluster.df$full_text)
  cluster.label <- get_label(cluster.word_freqs, master.label)
  cluster.center <- cluster.matrix[cluster.df$vector_type=="cluster_center",]
  cluster.nearest_center <- get_nearest_center(cluster.df, cluster.matrix, cluster.center)
  
  cluster.subclusters <- list()
  for (j in 1:cluster.k) {
    subcluster.df <- cluster.df[cluster.df$subcluster == j,]
    subcluster.matrix <- cluster.matrix[cluster.df$subcluster == j,]
    
    subcluster.word_freqs <- get_word_freqs(subcluster.df$full_text)
    subcluster.label <- get_label(subcluster.word_freqs, c(master.label, cluster.label))
    subcluster.center <- subcluster.matrix[subcluster.df$vector_type=="subcluster_center",]
    subcluster.nearest_center <- get_nearest_center(subcluster.df, subcluster.matrix, subcluster.center)
    
    cluster.subclusters[[j]] <- list(word_freqs=subcluster.word_freqs, label=subcluster.label, nearest_center=subcluster.nearest_center)
  }
  
  clusters[[i]] <- list(word_freqs=cluster.word_freqs, label=cluster.label, nearest_center=cluster.nearest_center, subclusters=cluster.subclusters)
}

```

```{r, echo=FALSE}
###############################################################################
# Run T-SNE on all the tweets and then again on each cluster to get
# plot coordinates for each tweet. We output a master plot with all clusters
# and a cluster plot with all subclusters for each cluster.
###############################################################################

set.seed(700)
tsne <- Rtsne(tweet.vectors.matrix, dims=2, perplexity=25, max_iter=750, check_duplicates=FALSE)
tsne.plot <- cbind(tsne$Y, tweet.vectors.df)
colnames(tsne.plot)[1:2] <- c("X", "Y")
tsne.plot$full_text <- sapply(tsne.plot$full_text, function(t) paste(strwrap(t ,width=60), collapse="<br>"))
tsne.plot$cluster.label <- sapply(tsne.plot$cluster, function(c) clusters[[c]]$label)

taglist <- htmltools::tagList()

#Master high level plot
fig <- plot_ly(tsne.plot, x=~X, y=~Y, 
               text=~paste("Cluster:", cluster, "<br>Class:", class, "<br>IsSpecificEvent:", is_specific_event, "<br>Opinion:", opinion, "<br>Text:", full_text), 
               color=~cluster.label, type="scatter", mode="markers")
fig <- fig %>% layout(title=paste("Master Plot:", master.label, "(high level clusters)"), 
                        yaxis=list(zeroline=FALSE), 
                        xaxis=list(zeroline=FALSE))
fig <- fig %>% toWebGL()
taglist[[1]] <- fig

#Cluster plots
plot_index <- 2
for (i in 1:k) {
  print(paste("Plotting cluster", i, "..."))
  cluster.matrix <- tweet.vectors.matrix[tsne.plot$cluster == i,]
  
  set.seed(900)
  cluster.tsne <- Rtsne(cluster.matrix, dims=2, perplexity=12, max_iter=500, check_duplicates=FALSE)
  cluster.tsne.plot <- cbind(cluster.tsne$Y, tsne.plot[tsne.plot$cluster == i,])
  colnames(cluster.tsne.plot)[1:2] <- c("cluster.X", "cluster.Y")
  cluster.tsne.plot$subcluster.label <- sapply(cluster.tsne.plot$subcluster, function(c) clusters[[i]]$subclusters[[c]]$label)
  
  #Cluster plot with regrouped positions by subcluster
  fig <- plot_ly(cluster.tsne.plot, x=~cluster.X, y=~cluster.Y, 
                 text=~paste("Subcluster:", subcluster, "<br>Class:", class, "<br>IsSpecificEvent:", is_specific_event, "<br>Opinion:", opinion, "<br>Text:", full_text), 
                 color=~subcluster.label, type="scatter", mode="markers")
  fig <- fig %>% layout(title=paste('Cluster ', i, ": ", clusters[[i]]$label, " (regrouped by subcluster)", sep=""), 
                        yaxis=list(zeroline=FALSE), 
                        xaxis=list(zeroline=FALSE))
  #fig <- fig %>% toWebGL()
  taglist[[plot_index]] <- fig
  plot_index <- plot_index + 1
}

```

\  

### Displaying `r nrow(results$df)` of `r results$total` results:

\  

```{r, echo=FALSE}
taglist
```

# Analysis

Present an analysis of the results obtained by your methods. For example:

* generate histograms, heat maps, or correlation plots that help illustrate how the data answers the research question.
* display samples of tweets within the theme clusters and topic subclusters that are helpful for context.


```{r, echo=FALSE}
n_nearest_neighbors <- 5

theme_caption <- paste(n_nearest_neighbors, "closest tweets to theme cluster center")
topic_caption <- paste(n_nearest_neighbors, "closest tweets to topic subcluster center")
```

## Theme (cluster 1): `r clusters[[1]]$label`

```{r, echo=FALSE}
kable(clusters[[1]]$nearest_center[1:n_nearest_neighbors,], caption=theme_caption) %>% kable_styling()
```

## Topic (subcluster 1.1): `r clusters[[1]]$subclusters[[3]]$label`

```{r, echo=FALSE}
kable(clusters[[1]]$subclusters[[3]]$nearest_center[1:n_nearest_neighbors,], caption=topic_caption) %>% kable_styling()
```

# Discussion

Summarize the study and discuss key takeways.

## Next steps for this analysis

Discuss interesting directions for follow-up investigation.

## Limitations

Discuss any technical limitations or general assumptions made in the study that the reader should be aware of.

# References

[1] ...

[2] ...

[3] ...